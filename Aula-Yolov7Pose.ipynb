{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":86280,"status":"ok","timestamp":1713309045843,"user":{"displayName":"arthur lima","userId":"06398003315782379190"},"user_tz":180},"id":"7aV1EcoC4JHO","outputId":"b3546963-5158-47d5-a21c-95485cb9247f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch in ./vision/lib/python3.10/site-packages (2.2.2)\n","Requirement already satisfied: filelock in ./vision/lib/python3.10/site-packages (from torch) (3.13.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in ./vision/lib/python3.10/site-packages (from torch) (4.11.0)\n","Requirement already satisfied: sympy in ./vision/lib/python3.10/site-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in ./vision/lib/python3.10/site-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in ./vision/lib/python3.10/site-packages (from torch) (3.1.3)\n","Requirement already satisfied: fsspec in ./vision/lib/python3.10/site-packages (from torch) (2024.3.1)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./vision/lib/python3.10/site-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./vision/lib/python3.10/site-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./vision/lib/python3.10/site-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./vision/lib/python3.10/site-packages (from torch) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./vision/lib/python3.10/site-packages (from torch) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./vision/lib/python3.10/site-packages (from torch) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./vision/lib/python3.10/site-packages (from torch) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./vision/lib/python3.10/site-packages (from torch) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./vision/lib/python3.10/site-packages (from torch) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./vision/lib/python3.10/site-packages (from torch) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./vision/lib/python3.10/site-packages (from torch) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in ./vision/lib/python3.10/site-packages (from torch) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in ./vision/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n","Requirement already satisfied: MarkupSafe>=2.0 in ./vision/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in ./vision/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: torchvision in ./vision/lib/python3.10/site-packages (0.17.2)\n","Collecting torchaudio\n","  Using cached torchaudio-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n","Requirement already satisfied: numpy in ./vision/lib/python3.10/site-packages (from torchvision) (1.23.5)\n","Requirement already satisfied: torch==2.2.2 in ./vision/lib/python3.10/site-packages (from torchvision) (2.2.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./vision/lib/python3.10/site-packages (from torchvision) (10.3.0)\n","Requirement already satisfied: filelock in ./vision/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (3.13.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in ./vision/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (4.11.0)\n","Requirement already satisfied: sympy in ./vision/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (1.12)\n","Requirement already satisfied: networkx in ./vision/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (3.3)\n","Requirement already satisfied: jinja2 in ./vision/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (3.1.3)\n","Requirement already satisfied: fsspec in ./vision/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (2024.3.1)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./vision/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./vision/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./vision/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./vision/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./vision/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./vision/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./vision/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./vision/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./vision/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./vision/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./vision/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in ./vision/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in ./vision/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2->torchvision) (12.4.127)\n","Requirement already satisfied: MarkupSafe>=2.0 in ./vision/lib/python3.10/site-packages (from jinja2->torch==2.2.2->torchvision) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in ./vision/lib/python3.10/site-packages (from sympy->torch==2.2.2->torchvision) (1.3.0)\n","Using cached torchaudio-2.2.2-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n","Installing collected packages: torchaudio\n","Successfully installed torchaudio-2.2.2\n","Requirement already satisfied: opencv-python in ./vision/lib/python3.10/site-packages (4.9.0.80)\n","Requirement already satisfied: numpy>=1.21.2 in ./vision/lib/python3.10/site-packages (from opencv-python) (1.23.5)\n","Requirement already satisfied: matplotlib in ./vision/lib/python3.10/site-packages (3.8.4)\n","Requirement already satisfied: contourpy>=1.0.1 in ./vision/lib/python3.10/site-packages (from matplotlib) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in ./vision/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in ./vision/lib/python3.10/site-packages (from matplotlib) (4.51.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in ./vision/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: numpy>=1.21 in ./vision/lib/python3.10/site-packages (from matplotlib) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in ./vision/lib/python3.10/site-packages (from matplotlib) (24.0)\n","Requirement already satisfied: pillow>=8 in ./vision/lib/python3.10/site-packages (from matplotlib) (10.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in ./vision/lib/python3.10/site-packages (from matplotlib) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in ./vision/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in ./vision/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Requirement already satisfied: numpy in ./vision/lib/python3.10/site-packages (1.23.5)\n"]}],"source":["!pip install torch\n","!pip install torchvision torchaudio\n","!pip install opencv-python\n","!pip install matplotlib\n","!pip install numpy"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1713309245786,"user":{"displayName":"arthur lima","userId":"06398003315782379190"},"user_tz":180},"id":"brzwequYVTQO"},"outputs":[],"source":["\"\"\"\n","Pytorch é uma biblioteca de código aberto que fornece uma ampla gama de algoritmos de Machine Learning.\n","Torchvision é uma biblioteca que pode ser usada para transformações de imagem comuns em visão computacional.\n","Letterbox é uma função definida no módulo datasets do pacote utils. É usado para redimensionar uma imagem para uma dimensão específica sem alterar o aspecto original da imagem.\n","\"\"\"\n","\n","# importa bibliotecas necessárias, incluindo torch para deep learning e transforms para pré-processamento de imagem\n","import torch\n","from torchvision import transforms\n","from utils.datasets import letterbox\n","from utils.draw_kpts import desenhar_keypoints\n","\n","# importa bibliotecas para manipulação de imagens e gráficos\n","import numpy as np\n","import cv2\n","\n","# importa bibliotecas para medição de tempo e interação com o sistema\n","import time\n","import sys"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1713309249271,"user":{"displayName":"arthur lima","userId":"06398003315782379190"},"user_tz":180},"id":"rGi8NZ4VSZt8","outputId":"f3c2716c-2fae-438d-d09d-ab335d0fcc85"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dispositivo: cpu\n"]}],"source":["# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device = torch.device(\"cpu\") # define o dispositivo de computação\n","\n","print(\"Dispositivo:\", device)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":171,"status":"ok","timestamp":1713309250430,"user":{"displayName":"arthur lima","userId":"06398003315782379190"},"user_tz":180},"id":"QRLSNDW5Fsgx","outputId":"8603c96c-0b26-4be4-cbb1-85b8685a1694"},"outputs":[{"name":"stdout","output_type":"stream","text":["Carregando modelo...\n","Modelo carregado!\n"]}],"source":["print(\"Carregando modelo...\")\n","\n","# Carrega o modelo YOLOv7 pré-treinado e o coloca em modo de avaliação\n","# modelo = torch.hub.load('WongKinYiu/yolov7', 'yolov7-w6-pose.pt', pretrained=True, trust_repo=True, force_reload=True).autoshape()\n","modelo = torch.load('yolov7-w6-pose.pt', map_location=torch.device(device))['model']\n","modelo = modelo.to(device).float().eval()\n","\n","print(\"Modelo carregado!\")"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":90,"status":"ok","timestamp":1713309252199,"user":{"displayName":"arthur lima","userId":"06398003315782379190"},"user_tz":180},"id":"M6fh_dIFFsyi","outputId":"1931e5a8-4114-49eb-9f82-58bf5c70a8ba"},"outputs":[{"name":"stdout","output_type":"stream","text":["Abrindo vídeo: ./dataset/video0.mp4\n"]}],"source":["# Abre um vídeo para processamento\n","video_path = './dataset/video0.mp4'\n","print(\"Abrindo vídeo:\", video_path)\n","cap = cv2.VideoCapture(video_path)"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":148,"status":"ok","timestamp":1713309253258,"user":{"displayName":"arthur lima","userId":"06398003315782379190"},"user_tz":180},"id":"L4o92mqXFs0h"},"outputs":[],"source":["if not cap.isOpened():\n","    print(\"Falha ao abrir o vídeo\")\n","    exit(1)\n","\n","# Lendo uma imagem para redimensiona-la\n","lido, imagem = cap.read()\n","\n","if not lido:\n","    sys.exit(1)"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1713309255385,"user":{"displayName":"arthur lima","userId":"06398003315782379190"},"user_tz":180},"id":"vK-7WbrTFs2h"},"outputs":[],"source":["# Redimensionando imagem, sem a afetar a quantidade de detalhes dela\n","imagem_reduzida = letterbox(imagem, 512, stride=64, auto=True)[0]\n","# Capturando dimensões para criar um VideoWriter com estas dimensões\n","altura, largura, _ = imagem_reduzida.shape"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1713309256083,"user":{"displayName":"arthur lima","userId":"06398003315782379190"},"user_tz":180},"id":"51y0XiDpFs4u"},"outputs":[],"source":["# Nome da imagem\n","nome_arquivo = f\"{video_path.split('/')[-1].split('.')[0]}\"\n","\n","# Definindo codec como MP4V\n","codec = cv2.VideoWriter_fourcc(*'mp4v')"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1713309257153,"user":{"displayName":"arthur lima","userId":"06398003315782379190"},"user_tz":180},"id":"Y5Ilw-vCF6kY"},"outputs":[],"source":["# Criando um VideoWriter para escrever vídeo de inferência com dimensôes da imaem que sofreu resize\n","output_video = cv2.VideoWriter(f\"./{nome_arquivo}_keypoints.mp4\", codec, 30, (largura, altura))"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1713309257829,"user":{"displayName":"arthur lima","userId":"06398003315782379190"},"user_tz":180},"id":"Vq_157BHF6mn","outputId":"7fd3d3ce-95ac-478e-c9cb-b8bff5dcc9ca"},"outputs":[{"name":"stdout","output_type":"stream","text":["Vídeo foi aberto e suas informações como, altura e largura das imagens, foram definidas com sucesso!\n","Começando inferência do modelo e escrita do vídeo de saída...\n"]}],"source":["print(\"Vídeo foi aberto e suas informações como, altura e largura das imagens, foram definidas com sucesso!\")\n","\n","print(\"Começando inferência do modelo e escrita do vídeo de saída...\")"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1713309259174,"user":{"displayName":"arthur lima","userId":"06398003315782379190"},"user_tz":180},"id":"YJ13FFdIF6or"},"outputs":[],"source":["# inicializa contadores\n","frame_count = 0 # Contador de frames\n","total_fps = 0 # Total de frames por segundo"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":62734,"status":"ok","timestamp":1713309322840,"user":{"displayName":"arthur lima","userId":"06398003315782379190"},"user_tz":180},"id":"TxLj_dznF6qV","outputId":"9420f2ad-bd66-4251-eb34-8ae995605967"},"outputs":[{"name":"stdout","output_type":"stream","text":["On frame: 0\n","On frame: 1\n","On frame: 2\n","On frame: 3\n","On frame: 4\n","On frame: 5\n","On frame: 6\n","On frame: 7\n","On frame: 8\n","On frame: 9\n","On frame: 10\n","On frame: 11\n","On frame: 12\n","On frame: 13\n","On frame: 14\n","On frame: 15\n","On frame: 16\n","On frame: 17\n","On frame: 18\n","On frame: 19\n","On frame: 20\n","On frame: 21\n","On frame: 22\n","On frame: 23\n","On frame: 24\n","On frame: 25\n","On frame: 26\n","On frame: 27\n","On frame: 28\n","On frame: 29\n","On frame: 30\n","On frame: 31\n","On frame: 32\n","On frame: 33\n","On frame: 34\n","On frame: 35\n","On frame: 36\n","On frame: 37\n","On frame: 38\n","On frame: 39\n","On frame: 40\n","On frame: 41\n","On frame: 42\n","On frame: 43\n","On frame: 44\n","On frame: 45\n","On frame: 46\n","On frame: 47\n","On frame: 48\n","On frame: 49\n","On frame: 50\n","On frame: 51\n","On frame: 52\n","On frame: 53\n","On frame: 54\n","On frame: 55\n","On frame: 56\n","On frame: 57\n","On frame: 58\n","On frame: 59\n","On frame: 60\n","On frame: 61\n","On frame: 62\n","On frame: 63\n","On frame: 64\n","On frame: 65\n","On frame: 66\n","On frame: 67\n","On frame: 68\n","On frame: 69\n","On frame: 70\n","On frame: 71\n","On frame: 72\n","On frame: 73\n","On frame: 74\n","On frame: 75\n","On frame: 76\n","On frame: 77\n","On frame: 78\n","On frame: 79\n","On frame: 80\n","On frame: 81\n","On frame: 82\n","On frame: 83\n","On frame: 84\n","On frame: 85\n","On frame: 86\n","On frame: 87\n","On frame: 88\n","On frame: 89\n","On frame: 90\n","On frame: 91\n","On frame: 92\n","On frame: 93\n","On frame: 94\n","On frame: 95\n","On frame: 96\n","On frame: 97\n","On frame: 98\n","On frame: 99\n","On frame: 100\n","On frame: 101\n","On frame: 102\n","On frame: 103\n","On frame: 104\n","On frame: 105\n","On frame: 106\n","On frame: 107\n","On frame: 108\n","On frame: 109\n","On frame: 110\n","On frame: 111\n","On frame: 112\n","On frame: 113\n","On frame: 114\n","On frame: 115\n","On frame: 116\n","On frame: 117\n","On frame: 118\n","On frame: 119\n","On frame: 120\n","On frame: 121\n","On frame: 122\n","On frame: 123\n","On frame: 124\n","On frame: 125\n","On frame: 126\n","On frame: 127\n","On frame: 128\n","On frame: 129\n","On frame: 130\n","On frame: 131\n","On frame: 132\n","On frame: 133\n","On frame: 134\n","On frame: 135\n","On frame: 136\n","On frame: 137\n","On frame: 138\n","On frame: 139\n","On frame: 140\n","On frame: 141\n","On frame: 142\n","On frame: 143\n","On frame: 144\n","On frame: 145\n","On frame: 146\n","On frame: 147\n","On frame: 148\n","On frame: 149\n","On frame: 150\n","On frame: 151\n","On frame: 152\n","On frame: 153\n","On frame: 154\n","On frame: 155\n","On frame: 156\n","On frame: 157\n","On frame: 158\n","On frame: 159\n","On frame: 160\n","On frame: 161\n","On frame: 162\n","On frame: 163\n","On frame: 164\n","On frame: 165\n","On frame: 166\n","On frame: 167\n","On frame: 168\n","On frame: 169\n","On frame: 170\n","On frame: 171\n","On frame: 172\n","On frame: 173\n","On frame: 174\n","On frame: 175\n","On frame: 176\n","On frame: 177\n","On frame: 178\n","On frame: 179\n","On frame: 180\n","On frame: 181\n","On frame: 182\n","On frame: 183\n","On frame: 184\n","On frame: 185\n","On frame: 186\n","On frame: 187\n","On frame: 188\n","On frame: 189\n","On frame: 190\n","On frame: 191\n","On frame: 192\n","On frame: 193\n","On frame: 194\n","On frame: 195\n","On frame: 196\n","On frame: 197\n","On frame: 198\n","On frame: 199\n","On frame: 200\n","On frame: 201\n","On frame: 202\n","On frame: 203\n","On frame: 204\n","On frame: 205\n","On frame: 206\n","On frame: 207\n","On frame: 208\n","On frame: 209\n","On frame: 210\n","On frame: 211\n","On frame: 212\n","On frame: 213\n","On frame: 214\n","On frame: 215\n","On frame: 216\n","On frame: 217\n","On frame: 218\n","On frame: 219\n","On frame: 220\n","On frame: 221\n","On frame: 222\n","On frame: 223\n","On frame: 224\n","On frame: 225\n","On frame: 226\n","On frame: 227\n","On frame: 228\n","On frame: 229\n","On frame: 230\n","On frame: 231\n","On frame: 232\n","On frame: 233\n","On frame: 234\n","On frame: 235\n","On frame: 236\n","On frame: 237\n","On frame: 238\n","On frame: 239\n","On frame: 240\n","On frame: 241\n","On frame: 242\n","On frame: 243\n","On frame: 244\n","On frame: 245\n","On frame: 246\n","On frame: 247\n","On frame: 248\n","On frame: 249\n","On frame: 250\n","On frame: 251\n","On frame: 252\n","On frame: 253\n","On frame: 254\n","On frame: 255\n","On frame: 256\n","On frame: 257\n","On frame: 258\n","On frame: 259\n","On frame: 260\n","On frame: 261\n","On frame: 262\n","On frame: 263\n","On frame: 264\n","On frame: 265\n","On frame: 266\n","On frame: 267\n","On frame: 268\n","On frame: 269\n","On frame: 270\n","On frame: 271\n","On frame: 272\n","On frame: 273\n","On frame: 274\n","On frame: 275\n","On frame: 276\n","On frame: 277\n","On frame: 278\n","On frame: 279\n","On frame: 280\n","On frame: 281\n","On frame: 282\n","On frame: 283\n","On frame: 284\n","On frame: 285\n","On frame: 286\n","On frame: 287\n","On frame: 288\n","On frame: 289\n","On frame: 290\n","On frame: 291\n","On frame: 292\n","On frame: 293\n","On frame: 294\n","On frame: 295\n","On frame: 296\n","On frame: 297\n","On frame: 298\n","On frame: 299\n","On frame: 300\n","On frame: 301\n","On frame: 302\n","On frame: 303\n","On frame: 304\n","On frame: 305\n","On frame: 306\n","On frame: 307\n","On frame: 308\n","On frame: 309\n","On frame: 310\n","On frame: 311\n","On frame: 312\n","On frame: 313\n","On frame: 314\n","On frame: 315\n","On frame: 316\n","On frame: 317\n","On frame: 318\n","On frame: 319\n","On frame: 320\n","On frame: 321\n","On frame: 322\n","On frame: 323\n","On frame: 324\n","On frame: 325\n","On frame: 326\n","On frame: 327\n","On frame: 328\n","On frame: 329\n","On frame: 330\n","On frame: 331\n","On frame: 332\n","On frame: 333\n","On frame: 334\n","On frame: 335\n","On frame: 336\n","On frame: 337\n","On frame: 338\n","On frame: 339\n","On frame: 340\n","On frame: 341\n","On frame: 342\n","On frame: 343\n","On frame: 344\n","On frame: 345\n","On frame: 346\n","On frame: 347\n","On frame: 348\n","On frame: 349\n","On frame: 350\n","Vídeo escrito com sucesso!\n"]}],"source":["while True:\n","    # Captura cada frame (frame) do video\n","    # ret é um bool que diz se o frame foi capturado ou não\n","    ret, frame = cap.read()\n","\n","    if not ret:\n","        break\n","\n","    # Capturando imagem original e convertendo seus canais para RGB\n","    # Passando imagem na LetterBox Para o Resize\n","    imagem_redimensionada = letterbox(frame, 512, stride=64, auto=True)[0] # shape: (567, 960, 3) HWC\n","\n","    # tensor -> # torch.Size([3, 567, 960]) CHW\n","    # unsqueeze(0) -> transformação para batch (lote), torch.Size([1, 3, 567, 960]) 1 -> tamanho lote 1 imagem\n","    # Float() -> float32, aumenta a precisão dos números, o que é bom para CPU\n","    imagem_tensor = transforms.ToTensor()(imagem_redimensionada).unsqueeze(0).to(device).float()\n","\n","    # Marca o tempo de início e posteriormente o fim da inferência para calcular FPS\n","    start_time = time.time()\n","\n","    print(\"On frame:\", frame_count)\n","\n","    # Realiza a detecção de pose usando o modelo YOLOv7\n","    with torch.no_grad():\n","        \"\"\"\n","        model(image) -> retorna, coordenadas das bounding boxes, class predictions (previsões), e\n","        confidencia (float) para cada objeto detectado na imagem\n","        \"\"\"\n","        saida, _ = modelo(imagem_tensor)\n","\n","    end_time = time.time()\n","\n","    # calculando FPS\n","    fps = 1 / (end_time - start_time)\n","    total_fps += fps\n","\n","    frame_count += 1\n","\n","    # Escreve keypoints detectados em cada frame\n","    imagem_com_kpts = desenhar_keypoints(modelo, saida, imagem_redimensionada)\n","\n","    # Escreve FPS em frame\n","    # image = image.numpy().astype(np.uint8)\n","    cv2.putText(imagem_com_kpts, f\"{fps:.1f} FPS\", (15, 30), cv2.FONT_HERSHEY_SIMPLEX,\n","                1, (0, 255, 0), 2)\n","\n","    # Escreve imagem no vídeo de output\n","    output_video.write(imagem_com_kpts)\n","\n","# Libera captura do video de output\n","cap.release()\n","\n","# Fecha todos os frames e janelas do video\n","# cv2.destroyAllWindows()\n","\n","print(\"Vídeo escrito com sucesso!\")"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1713309395251,"user":{"displayName":"arthur lima","userId":"06398003315782379190"},"user_tz":180},"id":"GP62mmmGGJ8K","outputId":"c357794b-4cdc-4d52-98a7-2d192728e2d1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Média de FPS: 6.3\n"]}],"source":["# Calcula e retorna o FPS\n","avg_fps = total_fps / frame_count\n","\n","print(f\"Média de FPS: {avg_fps:.1f}\")"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPerZN3RGaSCWWB3Akfl4II","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
